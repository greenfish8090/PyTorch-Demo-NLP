{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1591b2d4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677c341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e7164",
   "metadata": {},
   "source": [
    "# The vanilla RNN cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a29cf0",
   "metadata": {},
   "source": [
    "![image.png](images/rnn_equations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8486c3",
   "metadata": {},
   "source": [
    "# Inner workings of an RNN cell\n",
    "![image.png](images/rnn_anatomy.png)\n",
    "The biases b and c are omitted to simplify illustration. Also, Å· = softmax(o[t]) is omitted from the model anatomy because often, this is calculated after inference seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca53289",
   "metadata": {},
   "source": [
    "# The unrolled representation\n",
    "![image.png](images/rnn_unrolled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7e3e4",
   "metadata": {},
   "source": [
    "# The compact representation\n",
    "![image.png](images/rnn_compact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2649a369",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "To implement this in pytorch, we first need to make a module for RNN cell. Recap of the formulas:\n",
    "![image.png](images/rnn_equations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3210d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U = nn.Linear(in_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, x, hidden_prev):\n",
    "        # hidden_prev is h[t-1], x is x[t] in the formulas\n",
    "        \n",
    "        # self.W.forward(hidden) does the matrix multiplication W x hidden because W is a linear layer. Same with U\n",
    "        # Why aren't we taking account of the bias b mentioned in equation 1? You'll get a hint if you \n",
    "        # read the docs for torch.nn.Linear :)\n",
    "        a = self.W(hidden_prev) + self.U(x)\n",
    "        hidden_new = torch.tanh(a)\n",
    "        o = self.V(hidden_new)\n",
    "        \n",
    "        # As mentioned before, the softmax part can be done seperately from the model.\n",
    "        # Losses like torch.nn.CrossEntropyLoss expect the output to be un-softmaxed. It's more numerically stable.\n",
    "        return o, hidden_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cad082",
   "metadata": {},
   "source": [
    "Make an RNN cell with input size 2, hidden size 8 and output size 3, just like in the illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4734871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = RNNCell(2, 8, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943c5d2",
   "metadata": {},
   "source": [
    "# Single timestep inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23925515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.7923, -1.9647]), tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(2)\n",
    "hidden = torch.zeros(8)\n",
    "inp, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0866398a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1407,  0.1450,  0.2688], grad_fn=<AddBackward0>),\n",
       " tensor([ 0.4187,  0.7815,  0.2956, -0.7836, -0.8777,  0.5662, -0.7539,  0.4958],\n",
       "        grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, hidden_new = rnn_cell(inp, hidden)\n",
    "out, hidden_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5d844",
   "metadata": {},
   "source": [
    "# Multi timestep inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b914239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1621,  1.6706],\n",
       "        [ 0.2791,  0.5713],\n",
       "        [ 1.7893, -0.5475],\n",
       "        [-0.0800,  0.9690],\n",
       "        [ 1.0539,  0.0329],\n",
       "        [ 1.8258,  3.1347],\n",
       "        [ 0.6438, -0.0331],\n",
       "        [ 0.4236, -0.9052],\n",
       "        [-0.1745,  0.0764],\n",
       "        [-1.0459,  1.7758]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Say we have 10 timesteps of input\n",
    "inps = torch.randn(10, 2)\n",
    "inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "226e046a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1154, -0.2380,  0.0718],\n",
       "        [-0.0826, -0.3541,  0.3018],\n",
       "        [-0.4145, -0.0531,  0.2891],\n",
       "        [-0.5797, -0.2687,  0.0800],\n",
       "        [-0.3567, -0.2947,  0.3116],\n",
       "        [-0.5169, -0.5098,  0.1366],\n",
       "        [-0.3100, -0.2848,  0.3023],\n",
       "        [-0.4331, -0.0212,  0.1975],\n",
       "        [-0.4625, -0.1930,  0.1603],\n",
       "        [-0.3182, -0.4012,  0.1232]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = torch.empty(10, 3)\n",
    "hidden = torch.zeros(8)\n",
    "for timestep, inp in enumerate(inps):\n",
    "    # inp is a tensor of size 2\n",
    "    out, hidden = rnn_cell(inp, hidden)\n",
    "    outs[timestep] = out\n",
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aaa79c",
   "metadata": {},
   "source": [
    "# Abstract this into another Module?\n",
    "Looping through timesteps is annoying, so why don't we make it so that we call a module with all the timestep inputs and it returns the outputs generated by the RNNCell? Basically, we should be able to do `outs = rnn(inps)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f92b07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.rnn_cell = RNNCell(input_size, hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs is going to be of shape (timesteps x input_size) assuming that we are not batching.\n",
    "        # If the inputs are batched, then it will be of shape (batch_size x timesteps x input_size)\n",
    "        # In that case, the rhs of below line will be inputs.shape[1]\n",
    "        num_timesteps = inputs.shape[0]\n",
    "        outs = torch.empty(num_timesteps, self.out_size)\n",
    "        hidden = torch.zeros(self.hidden_size) # If batched, this should be torch.zeros(batch_size, self.hidden_size)\n",
    "        for timestep in range(num_timesteps):\n",
    "            # inp is a tensor of size 2\n",
    "            inp = inputs[timestep] # If batched, this should be inps[:, timestep, :]\n",
    "            out, hidden = rnn_cell(inp, hidden) # No change to this line when batched. Why?\n",
    "            outs[timestep] = out # If batched... you get the idea. Just add an extra dimension\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c29b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(2, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85dac96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1154, -0.2380,  0.0718],\n",
       "        [-0.0826, -0.3541,  0.3018],\n",
       "        [-0.4145, -0.0531,  0.2891],\n",
       "        [-0.5797, -0.2687,  0.0800],\n",
       "        [-0.3567, -0.2947,  0.3116],\n",
       "        [-0.5169, -0.5098,  0.1366],\n",
       "        [-0.3100, -0.2848,  0.3023],\n",
       "        [-0.4331, -0.0212,  0.1975],\n",
       "        [-0.4625, -0.1930,  0.1603],\n",
       "        [-0.3182, -0.4012,  0.1232]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn(inps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1be48",
   "metadata": {},
   "source": [
    "# The LSTM\n",
    "![image.png](images/lstm_cell.png)\n",
    "\n",
    "No, I am not going to implement this myself or subject you through that. Let's use PyTorch's own implementation\\\n",
    "\n",
    "\n",
    "## PyTorch's implementation\n",
    "Just like what we did here, pytorch exposes the API for both LSTMCell and LSTM. The semantics are the same: LSTMCell expects single timestep input, LSTM expects all the timesteps at once.\\\n",
    "nn.LSTM also implements [multiple layers](https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms) and [bidirectionality](https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms) which you'd otherwise have to do painstakingly if you only use the LSTMCell\n",
    "\n",
    "### Subtle differences\n",
    "1. There is no explicit `out_size`. At each timestep, the cell just gives you its hidden state. If you want the `out_size` to be different from `hidden_size`, you can pass the hidden state through a separate linear layer that goes from `hidden_size` -> `out_size`, or set `proj_size = out_size` in the nn.LSTM init params. It does the same thing.\n",
    "3. There are now 2 vectors: hidden state and cell state calculated at each timestep.\n",
    "2. Unlike our implementation, the nn.LSTM implementation returns the final timestep hidden_states and cell_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424cb46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size = 768,\n",
    "    hidden_size = 8,\n",
    "    num_layers = 2,\n",
    "    batch_first = True,\n",
    "    proj_size = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6fbc0",
   "metadata": {},
   "source": [
    "Let's say we are dealing with a 2 class classification problem. At each timestep, the input is a word and the model is supposed to predict whether that word falls into category 0 or 1.\\\n",
    "Input shape will be (`batch_size` x `timesteps` x `word_vector_dim`). (How many words in total?)\\\n",
    "Output shape will be (`batch_size` x `timesteps` x `2`)\\\n",
    "Ground truth labels would be of shape (`batch_size` x `timesteps`) where each element is 0 or 1, which category that particular word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f927817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(16, 10, 768) # batch_size x timesteps x input_size\n",
    "targets = torch.randint(0, 2, (16, 10)) # batch_size x timesteps\n",
    "h0 = torch.zeros(2, 16, 2) # num_layers x timesteps x out_size (= proj_size)\n",
    "c0 = torch.zeros(2, 16, 8)# num_layers x timesteps x hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c07e73ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10, 2]), torch.Size([2, 16, 2]), torch.Size([2, 16, 8]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, (hn, cn) = lstm(inputs, (h0, c0))\n",
    "outputs.shape, hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a102a9a",
   "metadata": {},
   "source": [
    "If we don't provide an initial (h, c) it assumes zero vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceeedf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10, 2]), torch.Size([2, 16, 2]), torch.Size([2, 16, 8]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, (hn, cn) = lstm(inputs)\n",
    "outputs.shape, hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d4c61",
   "metadata": {},
   "source": [
    "# Loss\n",
    "The loss function does not concern itself with differentiating between batches and timesteps. So, we combine batches and timesteps into one dimention before passing it to the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "147aa27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([160]), torch.Size([160, 2]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_targets = targets.view(-1)\n",
    "flattened_outputs = outputs.reshape(-1, 2)\n",
    "flattened_targets.shape, flattened_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05205f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6998, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(flattened_outputs, flattened_targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0877f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5c0fc",
   "metadata": {},
   "source": [
    "### Now that you can calculate the loss, the rest of the training pipeline is basically the same as for a regular feed forward network\n",
    "# The end!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
