{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69b0292",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c68062a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca2e7e",
   "metadata": {},
   "source": [
    "# Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbc7b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: data/mnist\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data/mnist\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mnist = torchvision.datasets.MNIST('data/mnist', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "small_train_mnist = torch.utils.data.Subset(train_mnist, np.random.randint(0, len(train_mnist), 10000))\n",
    "test_mnist = torchvision.datasets.MNIST('data/mnist', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "train_mnist, test_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e2c79a",
   "metadata": {},
   "source": [
    "# Dataset format\n",
    "Each instance of data (image) is a torch tensor of shape 1 x 28 x 28 where 28 is the width and height of each image and 1 is channel size. Since the dataset is Black/White,\n",
    "we have only one channel.\\\n",
    "Had this been a dataset of RGB images, there would be 3 channels, making each instance a tensor of shape 3 x 28 x 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a55659a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    image, label = test_mnist[i]\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(torchvision.transforms.ToPILImage()(image), cmap=\"Greys\")\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ecf41",
   "metadata": {},
   "source": [
    "# Dataloaders\n",
    "PyTorch uses dataloaders to load batches of data samples at once. Pretty convenient as we don't have to manually sample dataset instances and then collate them. For heavy datasets, you can set the num_workers argument in the dataloader which uses multiprocessing (Multiprocessing doesn't work in interactive notebooks though, you'll have to run it in a .py file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffdca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 28, 28]) torch.Size([8])\n",
      "torch.Size([8, 784])\n",
      "tensor([7, 6, 3, 8, 9, 2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_mnist,\n",
    "                                          batch_size=8,\n",
    "                                          shuffle=True)\n",
    "small_train_loader = torch.utils.data.DataLoader(small_train_mnist,\n",
    "                                          batch_size=8,\n",
    "                                          shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_mnist,\n",
    "                                          batch_size=8)\n",
    "for inputs, labels in train_loader:\n",
    "    batch_size = inputs.shape[0]\n",
    "    print(inputs.shape, labels.shape)\n",
    "    print(inputs.view(batch_size, -1).shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70317e9c",
   "metadata": {},
   "source": [
    "# Model\n",
    "Creating the PyTorch model\n",
    "![image.png](images/mnist_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc770775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(torch.nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) # Equivalent to self.fc1.forward(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        out = self.fc3(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d50d338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTModel(\n",
       "  (fc1): Linear(in_features=784, out_features=1568, bias=True)\n",
       "  (fc2): Linear(in_features=1568, out_features=1568, bias=True)\n",
       "  (fc3): Linear(in_features=1568, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MNISTModel(784, 1568, 10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4ff1f",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "Optimizers deal with updating the model using the model's gradients. Adam is one of the most popular ones which makes updates based on exponential moving averages of gradients and their squares. You don't need to worry too much about the theory, but if you're interested you can [read more](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af50995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr is the learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eadb530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to evaluate a model using a dataloader. This one calculates accuracy\n",
    "def test(model, loader):\n",
    "    # Set the model to eval mode. See line 20 in the next code block for more info\n",
    "    model.eval()\n",
    "    \n",
    "    # torchmetrics is a library that makes accuracy evaluation convenient. You can very easily do it manually as well\n",
    "    # using only tensor operations\n",
    "    accuracy = torchmetrics.Accuracy().to(device)\n",
    "    \n",
    "    # For each batch in the dataloader...\n",
    "    for inputs, labels in tqdm(loader, disable=True):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        \n",
    "        # Generate predictions. If you're wondering what torch.no_grad() is, it is a context manager that tells pytorch\n",
    "        # that we don't need to calculate gradients. Saves computation.\n",
    "        # We can only afford to do this here, not in the training\n",
    "        # process because, well, we need the gradients there.\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs.view(batch_size, -1))\n",
    "        accuracy.update(predictions, labels)\n",
    "    \n",
    "    return accuracy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b3a03",
   "metadata": {},
   "source": [
    "# The main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52181dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function with argument as number of epochs you want to train the model for\n",
    "def train(epochs): \n",
    "    # Just for plotting later\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar := tqdm(train_loader)):\n",
    "            pbar.set_description(f'Epoch {epoch}')\n",
    "            \n",
    "            # Test every 2500 batches. Call the accuracy calculating function for the model with the small_train_loader and\n",
    "            # test_loader\n",
    "            if batch_idx % 2500 == 0:\n",
    "                print(f'After {epoch*7500+batch_idx} batches{\" (No training)\" * (1 - bool(epoch+batch_idx))}:')\n",
    "                print(f'Train accuracy = {test(model, small_train_loader):.2%}')\n",
    "                print(f'Test accuracy = {test(model, test_loader):.2%}')\n",
    "                print('')\n",
    "                \n",
    "                # Set the model to train mode before exiting out of this if condition\n",
    "                # Read more: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "                # In actuality, our specific model has no layers that behave differently during training and testing, so this\n",
    "                # is unnecessary. It's just good practice.\n",
    "                model.train()\n",
    "                \n",
    "            # Send inputs and labels to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Flatten the inputs from shape (batch_size x 1 x 28 x 28) to shape (batch_size x 768)\n",
    "            # And get model predictions\n",
    "            batch_size = inputs.shape[0]\n",
    "            predictions = model(inputs.view(batch_size, -1)) # Recall that this is just model.forward(...)\n",
    "\n",
    "            # Calculate the loss using labels (Ground truth) and predictions\n",
    "            loss = torch.nn.functional.cross_entropy(predictions, labels)\n",
    "            \n",
    "            # Just for plotting later\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Clear out residual model parameter gradients from the previous iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate new gradients for this batch\n",
    "            # ∂L/∂W for all the weights W\n",
    "            # ∂L/∂b for all the biases b\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model using the gradients and the learning rate α, in essence:\n",
    "            # W <- W + α * ∂L/∂W for all the weights W\n",
    "            # b <- b + α * ∂L/∂b for all the biases b\n",
    "            # But of course, Adam does something slightly more sophisticated as mentioned before\n",
    "            optimizer.step()\n",
    "    \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10922cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541c3028d98c4cafb1b09d0b856f56b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 batches (No training):\n",
      "Train accuracy = 9.55%\n",
      "Test accuracy = 9.34%\n",
      "\n",
      "After 2500 batches:\n",
      "Train accuracy = 87.17%\n",
      "Test accuracy = 88.58%\n",
      "\n",
      "After 5000 batches:\n",
      "Train accuracy = 89.76%\n",
      "Test accuracy = 90.76%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e0ab7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7282f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting moving averages instead of raw noisy losses:\n",
    "# https://stackoverflow.com/questions/11352047/finding-moving-average-from-data-points-in-python\n",
    "window_width = 500\n",
    "cumsum_vec = np.cumsum(np.insert(train_losses, 0, 0)) \n",
    "ma_vec = (cumsum_vec[window_width:] - cumsum_vec[:-window_width]) / window_width\n",
    "plt.plot(ma_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    image, label = test_mnist[i]\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    with torch.no_grad():\n",
    "        pred = model(image.view(-1).to(device))\n",
    "    plt.imshow(torchvision.transforms.ToPILImage()(image), cmap=\"Greys\")\n",
    "    plt.title(f\"Prediction: {torch.argmax(pred)} ({torch.max(torch.nn.functional.softmax(pred, dim=0)):.1%})\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310d6a8",
   "metadata": {},
   "source": [
    "# The end!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
