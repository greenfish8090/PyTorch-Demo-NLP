{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775ae076",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014b1c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33bccd",
   "metadata": {},
   "source": [
    "# The vanilla RNN cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e583249",
   "metadata": {},
   "source": [
    "![image.png](images/rnn_equations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c689f4",
   "metadata": {},
   "source": [
    "# Inner workings of an RNN cell\n",
    "![image.png](images/rnn_anatomy.png)\n",
    "The biases b and c are omitted to simplify illustration. Also, Å· = softmax(o[t]) is omitted from the model anatomy because often, this is calculated after inference seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b0bd5",
   "metadata": {},
   "source": [
    "# The unrolled representation\n",
    "![image.png](images/rnn_unrolled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4413f9",
   "metadata": {},
   "source": [
    "# The compact representation\n",
    "![image.png](images/rnn_compact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a102d1",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "To implement this in pytorch, we first need to make a module for RNN cell. Recap of the formulas:\n",
    "![image.png](images/rnn_equations.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546286f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U = nn.Linear(in_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, x, hidden_prev):\n",
    "        # hidden_prev is h[t-1], x is x[t] in the formulas\n",
    "        \n",
    "        # self.W.forward(hidden) does the matrix multiplication W x hidden because W is a linear layer. Same with U\n",
    "        # Why aren't we taking account of the bias b mentioned in equation 1? You'll get a hint if you \n",
    "        # read the docs for torch.nn.Linear :)\n",
    "        a = self.W(hidden_prev) + self.U(x)\n",
    "        hidden_new = torch.tanh(a)\n",
    "        o = self.V(hidden_new)\n",
    "        \n",
    "        # As mentioned before, the softmax part can be done seperately from the model.\n",
    "        # Losses like torch.nn.CrossEntropyLoss expect the output to be un-softmaxed. It's more numerically stable.\n",
    "        return o, hidden_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3355b7",
   "metadata": {},
   "source": [
    "Make an RNN cell with input size 2, hidden size 8 and output size 3, just like in the illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b8b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = RNNCell(2, 8, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0521d",
   "metadata": {},
   "source": [
    "# Single timestep inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8ab279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.3930, -1.6565]), tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(2)\n",
    "hidden = torch.zeros(8)\n",
    "inp, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe50b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1109, 0.3269, 0.5143], grad_fn=<AddBackward0>),\n",
       " tensor([-0.5866, -0.0064, -0.5363,  0.1603, -0.9621, -0.1003, -0.4245, -0.7994],\n",
       "        grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, hidden_new = rnn_cell(inp, hidden)\n",
    "out, hidden_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d1cd8",
   "metadata": {},
   "source": [
    "# Multi timestep inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "639500e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6368, -0.2283],\n",
       "        [ 0.4639, -0.5682],\n",
       "        [ 1.6815,  0.9894],\n",
       "        [-0.4412,  0.8356],\n",
       "        [ 0.1152, -1.1925],\n",
       "        [-0.6684,  2.5875],\n",
       "        [-0.2061, -0.6215],\n",
       "        [-1.1522, -0.0372],\n",
       "        [-0.8357,  0.2232],\n",
       "        [ 1.5939, -1.0374]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Say we have 10 timesteps of input\n",
    "inps = torch.randn(10, 2)\n",
    "inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdde2725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2904, -0.2881, -0.2762],\n",
       "        [ 0.0617,  0.0259,  0.0087],\n",
       "        [-0.3076, -0.5146, -0.8079],\n",
       "        [ 0.3520,  0.1135, -0.3946],\n",
       "        [-0.0748,  0.0352,  0.3587],\n",
       "        [ 0.0010, -0.3069, -0.8104],\n",
       "        [ 0.1879,  0.2230,  0.1065],\n",
       "        [ 0.3138,  0.2054,  0.1739],\n",
       "        [ 0.2555,  0.1189, -0.0578],\n",
       "        [-0.3321, -0.1872,  0.1412]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = torch.empty(10, 3)\n",
    "hidden = torch.zeros(8)\n",
    "for timestep, inp in enumerate(inps):\n",
    "    # inp is a tensor of size 2\n",
    "    out, hidden = rnn_cell(inp, hidden)\n",
    "    outs[timestep] = out\n",
    "outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb178e80",
   "metadata": {},
   "source": [
    "# Abstract this into another Module?\n",
    "Looping through timesteps is annoying, so why don't we make it so that we call a module with all the timestep inputs and it returns the outputs generated by the RNNCell? Basically, we should be able to do `outs = rnn(inps)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768ba928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.rnn_cell = RNNCell(input_size, hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs is going to be of shape (timesteps x input_size) assuming that we are not batching.\n",
    "        # If the inputs are batched, then it will be of shape (batch_size x timesteps x input_size)\n",
    "        # In that case, the rhs of below line will be inputs.shape[1]\n",
    "        num_timesteps = inputs.shape[0]\n",
    "        outs = torch.empty(num_timesteps, self.out_size)\n",
    "        hidden = torch.zeros(self.hidden_size) # If batched, this should be torch.zeros(batch_size, self.hidden_size)\n",
    "        for timestep in range(num_timesteps):\n",
    "            # inp is a tensor of size 2\n",
    "            inp = inputs[timestep] # If batched, this should be inps[:, timestep, :]\n",
    "            out, hidden = rnn_cell(inp, hidden) # No change to this line when batched. Why?\n",
    "            outs[timestep] = out # If batched... you get the idea. Just add an extra dimension\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea1afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(2, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba8b67e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2904, -0.2881, -0.2762],\n",
       "        [ 0.0617,  0.0259,  0.0087],\n",
       "        [-0.3076, -0.5146, -0.8079],\n",
       "        [ 0.3520,  0.1135, -0.3946],\n",
       "        [-0.0748,  0.0352,  0.3587],\n",
       "        [ 0.0010, -0.3069, -0.8104],\n",
       "        [ 0.1879,  0.2230,  0.1065],\n",
       "        [ 0.3138,  0.2054,  0.1739],\n",
       "        [ 0.2555,  0.1189, -0.0578],\n",
       "        [-0.3321, -0.1872,  0.1412]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn(inps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86050b",
   "metadata": {},
   "source": [
    "# The LSTM\n",
    "![image.png](images/lstm_cell.png)\n",
    "\n",
    "No, I am not going to implement this myself or subject you through that. Let's use PyTorch's own implementation\\\n",
    "\n",
    "\n",
    "## PyTorch's implementation\n",
    "Just like what we did here, pytorch exposes the API for both LSTMCell and LSTM. The semantics are the same: LSTMCell expects single timestep input, LSTM expects all the timesteps at once.\\\n",
    "nn.LSTM also implements [multiple layers](https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms) and [bidirectionality](https://stats.stackexchange.com/questions/163304/what-are-the-advantages-of-stacking-multiple-lstms) which you'd otherwise have to do painstakingly if you only use the LSTMCell\n",
    "\n",
    "### Subtle differences\n",
    "1. There is no explicit `out_size`. At each timestep, the cell just gives you its hidden state. If you want the `out_size` to be different from `hidden_size`, you can pass the hidden state through a separate linear layer that goes from `hidden_size` -> `out_size`, or set `proj_size = out_size` in the nn.LSTM init params. It does the same thing.\n",
    "3. There are now 2 vectors: hidden state and cell state calculated at each timestep.\n",
    "2. Unlike our implementation, the nn.LSTM implementation returns the final timestep hidden_states and cell_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac4c3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size = 768,\n",
    "    hidden_size = 8,\n",
    "    num_layers = 2,\n",
    "    batch_first = True,\n",
    "    proj_size = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51aed00",
   "metadata": {},
   "source": [
    "Let's say we are dealing with a 2 class classification problem. At each timestep, the input is a word and the model is supposed to predict whether that word falls into category 0 or 1.\\\n",
    "Input shape will be (`batch_size` x `timesteps` x `word_vector_dim`). (How many words in total?)\\\n",
    "Output shape will be (`batch_size` x `timesteps` x `2`)\\\n",
    "Ground truth labels would be of shape (`batch_size` x `timesteps`) where each element is 0 or 1, which category that particular word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd893854",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(16, 10, 768) # batch_size x timesteps x input_size\n",
    "targets = torch.randint(0, 2, (16, 10)) # batch_size x timesteps\n",
    "h0 = torch.zeros(2, 16, 2) # num_layers x timesteps x out_size (= proj_size)\n",
    "c0 = torch.zeros(2, 16, 8)# num_layers x timesteps x hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4dcba18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10, 2]), torch.Size([2, 16, 2]), torch.Size([2, 16, 8]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs, (hn, cn) = lstm(inputs, (h0, c0))\n",
    "outputs.shape, hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda432c",
   "metadata": {},
   "source": [
    "If we don't provide an initial (h, c) it assumes zero vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e50506a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 10, 2]), torch.Size([2, 16, 2]), torch.Size([2, 16, 8]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, (hn, cn) = lstm(inputs)\n",
    "outputs.shape, hn.shape, cn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f7b605",
   "metadata": {},
   "source": [
    "# Loss\n",
    "The loss function does not concern itself with differentiating between batches and timesteps. So, we combine batches and timesteps into one dimention before passing it to the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b0ab734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([160]), torch.Size([160, 2]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_targets = targets.view(-1)\n",
    "flattened_outputs = outputs.reshape(-1, 2)\n",
    "flattened_targets.shape, flattened_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35a11619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6930, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(flattened_outputs, flattened_targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08c1a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b282a2a",
   "metadata": {},
   "source": [
    "### Now that you can calculate the loss, the rest of the training pipeline is basically the same as for a regular feed forward network\n",
    "# The end!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
